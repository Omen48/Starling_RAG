{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.2.61  # https://github.com/abetlen/llama-cpp-python, CUBLAS is for GPU usage\n",
        "!pip -q install  transformers langchain"
      ],
      "metadata": {
        "id": "XOhrZuwfBvQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!wget -O model.gguf https://huggingface.co/TheBloke/Starling-LM-7B-alpha-GGUF/resolve/main/starling-lm-7b-alpha.Q5_K_M.gguf  # сама gguf-модель, скачиваем с huggingface"
      ],
      "metadata": {
        "id": "5nRnwmbhGUrw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from llama_cpp import Llama\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n"
      ],
      "metadata": {
        "id": "AVwZGLPMEkHY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = Llama(\n",
        "  model_path=\"model.gguf\",\n",
        "  n_ctx=8192,  # длина контекста, воспринимаемого моделью # можно менять\n",
        "  n_threads=8,\n",
        "  n_gpu_layers=-1  # сколько слоёв GPU сгружать, -1 значит все. Starling влезает весь, чего не скажешь, например, о Mixtral\n",
        ")\n"
      ],
      "metadata": {
        "id": "QCy_JKuHQtEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# гиперпараметры для генерации\n",
        "llm_kwargs = {\n",
        "    'max_tokens': 2048,\n",
        "    'stop': [\"<|end_of_turn|>\"],\n",
        "    'temperature': 1.5,\n",
        "    'top_p': 1\n",
        "}"
      ],
      "metadata": {
        "id": "6rHp_1GuNJEM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "sent_model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "id": "eIF2yl3DQr7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prompt(question, text):\n",
        "    return f\"\"\"GPT4 Correct User: {question}\n",
        "    <text>{text}</text><|end_of_turn|>\n",
        "    GPT4 Correct Assistant:\"\"\"\n",
        "\n",
        "def generate_answer(prompt):\n",
        "    global llm\n",
        "    with torch.no_grad():\n",
        "        output = llm(prompt, **llm_kwargs)['choices'][0]['text'].strip()\n",
        "    return output\n",
        "\n",
        "def generate_answer_with_chunking(question, file_path, chunk_size=512):\n",
        "    answers = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        for chunk in read_in_chunks(file, chunk_size):\n",
        "            chunk = chunk.replace(\"{\", \"{{\").replace(\"}\", \"}}\")  # Escape characters right before use\n",
        "            message = generate_prompt(question, chunk)\n",
        "            with torch.no_grad():\n",
        "                answer = generate_answer(message, verbose=False, **llm_kwargs)\n",
        "            answers.append(answer)\n",
        "    return \" \".join(answers)\n"
      ],
      "metadata": {
        "id": "ctqvxwSkL0as"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(sentence):\n",
        "    encoded_input = sent_tokenizer([sentence], padding=True, truncation=True, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        model_output = sent_model(**encoded_input)\n",
        "    token_embeddings = model_output[0]\n",
        "    input_mask_expanded = encoded_input['attention_mask'].unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
      ],
      "metadata": {
        "id": "WqmNCV7xQ20L"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_in_chunks(file_object, chunk_size=512):\n",
        "    while True:\n",
        "        data = file_object.read(chunk_size)\n",
        "        if not data:\n",
        "            break\n",
        "        yield data"
      ],
      "metadata": {
        "id": "NqpTFjMQQ6oU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция для вычисления эмбеддингов файла\n",
        "def get_embeddings_from_file(file_path):\n",
        "    embeddings = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for chunk in read_in_chunks(f):\n",
        "            if not chunk.strip():  # Игнорируем пустые части\n",
        "                continue\n",
        "            chunk_embeddings = get_embedding(chunk)\n",
        "            embeddings.append(chunk_embeddings)\n",
        "    if embeddings:\n",
        "        final_embedding = torch.mean(torch.stack(embeddings), dim=0)\n",
        "        print(f\"Embedding for {file_path} calculated successfully.\")\n",
        "        return final_embedding\n",
        "    else:\n",
        "        print(f\"No valid embeddings found for {file_path}.\")\n",
        "        return torch.zeros_like()\n"
      ],
      "metadata": {
        "id": "mTws-fiWMg93"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_most_relevant_file(question_embedding):\n",
        "    max_similarity = -1  # Инициализация максимального сходства как очень маленькое значение\n",
        "    relevant_file_path = None  # Инициализация пути к наиболее релевантному файлу\n",
        "\n",
        "    # Проход по всем файлам и их эмбеддингам\n",
        "    for file_path, embedding in file_embeddings.items():\n",
        "        if embedding.nelement() == 0:\n",
        "            continue  # Пропускаем файлы с нулевыми эмбеддингами\n",
        "\n",
        "        # Убедимся, что эмбеддинги имеют размерность (1, D), где D - размер эмбеддинга\n",
        "        embedding = embedding.unsqueeze(0) if embedding.dim() == 1 else embedding\n",
        "        question_embedding = question_embedding.unsqueeze(0) if question_embedding.dim() == 1 else question_embedding\n",
        "\n",
        "        # Вычисление косинусного сходства между эмбеддингом файла и эмбеддингом вопроса\n",
        "        similarity = F.cosine_similarity(embedding, question_embedding, dim=1)\n",
        "\n",
        "        # Проверяем, что размерность similarity == (1,)\n",
        "        if similarity.size(0) != 1:\n",
        "            raise ValueError(f\"Expected similarity to be a scalar, got {similarity.size()}\")\n",
        "\n",
        "        similarity_value = similarity.item()\n",
        "\n",
        "        print(f\"File: {file_path}, Similarity: {similarity_value}\")  # Для отладки\n",
        "        if similarity_value > max_similarity:\n",
        "            max_similarity = similarity_value\n",
        "            relevant_file_path = file_path\n",
        "\n",
        "    return relevant_file_path\n"
      ],
      "metadata": {
        "id": "9IUsw-zogDZ3"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add your document file path\n",
        "file_paths = [ 'txt1.txt', 'txt2.txt', 'txt3.txt', 'txt4.txt']\n",
        "file_embeddings = {path: get_embeddings_from_file(path) for path in file_paths}"
      ],
      "metadata": {
        "id": "ga7NsedFM0m2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer_for_question(question):\n",
        "    question_embedding = get_embedding(question)\n",
        "    relevant_file_path = find_most_relevant_file(question_embedding)\n",
        "    if relevant_file_path:\n",
        "        with open(relevant_file_path, 'r', encoding='utf-8') as file:\n",
        "            full_text = file.read()\n",
        "        prompt = generate_prompt(question, full_text)\n",
        "        answer = generate_answer(prompt)\n",
        "        return answer, relevant_file_path\n",
        "    else:\n",
        "        return \"No relevant information found.\", None\n"
      ],
      "metadata": {
        "id": "RecrHL-FZBVk"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\"Перечисли основные виды ресурсов, такие как Информация, Организованность и тд и дай им определения?\",\"Что такое система? Базовое определение\", \"Напиши определение системы\", \"Что такое «Цель»?\"]"
      ],
      "metadata": {
        "id": "T34xZoHJi3lX"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for question in questions:\n",
        "    answer, file_path = generate_answer_for_question(question)\n",
        "    print(f\"Вопрос: {question}\")\n",
        "    if file_path:\n",
        "        print(f\"Ответ в файле: {file_path}\")\n",
        "    print(\"ответ:\", answer)\n",
        "    print()"
      ],
      "metadata": {
        "id": "KeduLVXbZELS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}